const ffmpeg = require('fluent-ffmpeg');
const fs = require('fs-extra');
const path = require('path');
const { exec } = require('child_process');

// ================= –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø =================
const INPUT_FILE = process.argv[2] || 'audio.mp3';
const OUTPUT_FILE = 'output_dictation.mp3';
const TRANSCRIPT_FILE = 'transcript.txt';

const REPEAT_COUNT = 3;
const PAUSE_BETWEEN_REPEATS = 3;

// –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–∞ (—Ñ—ñ–ª—å—Ç—Ä—É—î —à—É–º)
const MIN_SEGMENT_LENGTH = 0.4;

// Whisper: –æ–ø–∏—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –≤—ñ–¥–µ–æ (–ø–æ–∫—Ä–∞—â—É—î —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è)
// const WHISPER_PROMPT = "Kerri shares her special recipe for making a delicious omelet.";
const WHISPER_PROMPT = "Steven looks at a picture of a big red bus and talks about it.";
// ================================================

const TEMP_DIR = path.join(__dirname, 'temp_segments');
const SILENCE_FILE = path.join(TEMP_DIR, 'silence.mp3');

const run = async () => {
    if (!fs.existsSync(INPUT_FILE)) {
        console.error(`‚ùå Error: File "${INPUT_FILE}" not found.`);
        process.exit(1);
    }

    try {
        console.time('Processing Time');
        await fs.emptyDir(TEMP_DIR);

        console.log('üïµÔ∏è  1. Analyzing audio format...');
        const audioFormat = await getAudioFormat(INPUT_FILE);

        console.log('üîá 2. Generating matching silence...');
        await generateSilenceFile(PAUSE_BETWEEN_REPEATS, SILENCE_FILE, audioFormat);

        console.log('üîç 3. Detecting phrases using Whisper AI...');
        const segments = await detectSegments(INPUT_FILE);

        // –§—ñ–ª—å—Ç—Ä—É—î–º–æ –∑–æ–≤—Å—ñ–º —Å–º—ñ—Ç—Ç—è, –∞–ª–µ –∑–∞–ª–∏—à–∞—î–º–æ –∫–æ—Ä–æ—Ç–∫—ñ —Å–ª–æ–≤–∞
        const validSegments = segments.filter(s => (s.duration === null || s.duration > MIN_SEGMENT_LENGTH));

        console.log(`‚úÖ Found ${segments.length} raw segments.`);
        console.log(`üëâ Kept ${validSegments.length} segments after filtering noise.`);

        if (validSegments.length < 2) {
            console.warn("‚ö†Ô∏è WARNING: Found very few segments. Check audio quality or try a larger Whisper model");
        }

        console.log('üìÑ 4. Saving transcript...');
        saveTranscript(validSegments, TRANSCRIPT_FILE);

        console.log('‚úÇÔ∏è  5. Splitting audio...');
        const segmentFiles = await splitAudio(INPUT_FILE, validSegments, audioFormat);

        console.log('üìù 6. Building playlist...');
        const concatListPath = createConcatList(segmentFiles);

        console.log('üíæ 7. Merging final file...');
        await mergeAudio(concatListPath, OUTPUT_FILE, audioFormat);

        console.log('üßπ 8. Cleanup...');
        await fs.remove(TEMP_DIR);

        console.log(`üéâ Done!`);
        console.log(`   Audio: ${OUTPUT_FILE}`);
        console.log(`   Transcript: ${TRANSCRIPT_FILE}`);
        console.timeEnd('Processing Time');

    } catch (err) {
        console.error('‚ùå Error:', err);
    }
};

// --- CORE FUNCTIONS ---

function getAudioFormat(file) {
    return new Promise((resolve, reject) => {
        ffmpeg.ffprobe(file, (err, metadata) => {
            if (err) return reject(err);
            const stream = metadata.streams.find(s => s.codec_type === 'audio');
            if (!stream) return reject(new Error('No audio stream found'));
            resolve({
                sampleRate: stream.sample_rate || 44100,
                channels: stream.channels || 2,
                bit_rate: stream.bit_rate || '128k' // Get original bitrate
            });
        });
    });
}

function generateSilenceFile(duration, outputPath, format) {
    return new Promise((resolve, reject) => {
        const layout = format.channels === 1 ? 'mono' : 'stereo';
        ffmpeg()
            .input(`anullsrc=r=${format.sampleRate}:cl=${layout}`)
            .inputFormat('lavfi')
            .duration(duration)
            .output(outputPath)
            .on('end', resolve)
            .on('error', reject)
            .run();
    });
}

function detectSegments(file) {
    return new Promise((resolve, reject) => {
        const whisperScript = path.join(__dirname, 'whisper_detector.py');
        const pythonPath = path.join(__dirname, 'venv', 'bin', 'python3');
        const cmd = `"${pythonPath}" "${whisperScript}" "${file}" large "${WHISPER_PROMPT}"`;

        console.log('   (This may take a minute on first run - downloading model...)');
        console.log(`   Context: "${WHISPER_PROMPT}"`);

        exec(cmd, { maxBuffer: 10 * 1024 * 1024 }, (error, stdout, stderr) => {
            if (error) {
                console.error('Whisper Error:', stderr);
                return reject(new Error(`Whisper detection failed: ${error.message}`));
            }

            try {
                const result = JSON.parse(stdout);

                if (!result.success) {
                    return reject(new Error(`Whisper error: ${result.error}`));
                }

                // Convert Whisper segments to our format
                const segments = result.segments.map(seg => ({
                    start: seg.start,
                    duration: seg.duration,
                    text: seg.text  // –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ç–µ–∫—Å—Ç –¥–ª—è –º–æ–∂–ª–∏–≤–æ–≥–æ –≤–∏–≤–æ–¥—É
                }));

                if (segments.length === 0) {
                    console.warn('‚ö†Ô∏è  No speech detected by Whisper!');
                    resolve([{ start: 0, duration: null }]);
                    return;
                }

                console.log(`   üìù Transcription preview: "${segments[0].text}..."`);
                resolve(segments);
            } catch (parseError) {
                reject(new Error(`Failed to parse Whisper output: ${parseError.message}`));
            }
        });
    });
}

async function splitAudio(inputFile, segments, format) {
    const files = [];
    for (let i = 0; i < segments.length; i++) {
        const seg = segments[i];
        const fileName = path.join(TEMP_DIR, `seg_${i.toString().padStart(3, '0')}.mp3`);

        await new Promise((resolve, reject) => {
            let command = ffmpeg(inputFile).setStartTime(seg.start);
            if (seg.duration) command.setDuration(seg.duration);

            command
                .output(fileName)
                .audioCodec('libmp3lame')
                .audioFrequency(parseInt(format.sampleRate))
                .audioChannels(format.channels)
                .on('end', resolve)
                .on('error', reject)
                .run();
        });
        files.push(fileName);
    }
    return files;
}

function saveTranscript(segments, outputPath) {
    let content = '# Transcript with Timestamps\n\n';

    segments.forEach((seg, i) => {
        const startTime = formatTime(seg.start);
        const endTime = formatTime(seg.start + seg.duration);
        const text = seg.text || '[No text]';

        content += `[${startTime} - ${endTime}] ${text}\n`;
    });

    fs.writeFileSync(outputPath, content, 'utf-8');
}

function formatTime(seconds) {
    const mins = Math.floor(seconds / 60);
    const secs = (seconds % 60).toFixed(2);
    return `${mins.toString().padStart(2, '0')}:${secs.padStart(5, '0')}`;
}

function createConcatList(files) {
    const listPath = path.join(TEMP_DIR, 'list.txt');
    let content = '';
    files.forEach(f => {
        for (let i = 0; i < REPEAT_COUNT; i++) {
            content += `file '${f}'\n`;
            content += `file '${SILENCE_FILE}'\n`;
        }
    });
    fs.writeFileSync(listPath, content);
    return listPath;
}

function mergeAudio(listPath, output, format) {
    return new Promise((resolve, reject) => {
        ffmpeg()
            .input(listPath)
            .inputOptions(['-f concat', '-safe 0'])
            .audioCodec('libmp3lame')
            .audioBitrate(format.bit_rate)
            .save(output)
            .on('end', resolve)
            .on('error', reject);
    });
}

run();
